{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "43a47cbf8391b3ad19eb6e4412e9a0df564a28f628205ac6247f55929365ed7c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Perceptron\n",
    "\n",
    "Perceptron is the most basic type of neural networks. It concsists of nodes connected with weighted connections to an activation function.\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "      1 & \\sigma (w \\cdot x + b) > \\theta\\\\\n",
    "      0 & \\text{otherwise}\n",
    "\\end{cases}  \n",
    "$$\n",
    "\n",
    "where $w$, $x$, $b$, and $\\theta$ are the weights, inputs, the bias, and the threshold respectively. After each pass, the weights are updated according to Hebb's Rule as follows:\n",
    "\n",
    "$$\n",
    "w_{new} = w_{old} + \\Delta w \\\\\n",
    "\\Delta w = \\alpha \\cdot x \\cdot (t - f(x))\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $t$ is the target."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_features, lr, thresh):\n",
    "        self.num_features = num_features\n",
    "        self.w = [random.uniform(-1,1) for i in range(num_features)]\n",
    "        self.b = random.uniform(-1,1)\n",
    "        self.lr = lr\n",
    "        self.thresh = thresh\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        f_x = np.sum(np.multiply(x, self.w)) + self.b\n",
    "        return  1 if self.sigmoid(f_x) >= self.thresh else 0\n",
    "\n",
    "    def update_weights(self, x, loss):\n",
    "        for i in range(len(self.w)):\n",
    "            delta_w = self.lr*x[i]*loss\n",
    "            self.w[i] = self.w[i] + delta_w"
   ]
  },
  {
   "source": [
    "## Application\n",
    "\n",
    "We can use perceptrons to learn OR gates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "labels = [0, 1, 1, 1]\n",
    "model = Perceptron(num_features = 2, lr=5, thresh= 0.5)"
   ]
  },
  {
   "source": [
    "### Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for j, x in enumerate(inputs):\n",
    "        output = model.forward(inputs[j])\n",
    "        loss = labels[j] - output\n",
    "        model.update_weights(x, loss)"
   ]
  },
  {
   "source": [
    "### Testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model Weights: [5.858933138118771, 4.378180934967585]\nINPUT: [0, 0] --- TARGET: 0 --- OUTPUT: 0\nINPUT: [0, 1] --- TARGET: 1 --- OUTPUT: 1\nINPUT: [1, 0] --- TARGET: 1 --- OUTPUT: 1\nINPUT: [1, 1] --- TARGET: 1 --- OUTPUT: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model Weights: {model.w}\")\n",
    "for i in range(len(inputs)):\n",
    "    x, t = inputs[i], labels[i]\n",
    "    pred = model.forward(x)\n",
    "    print(f\"INPUT: {x} --- TARGET: {t} --- OUTPUT: {pred}\")"
   ]
  }
 ]
}