{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "43a47cbf8391b3ad19eb6e4412e9a0df564a28f628205ac6247f55929365ed7c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Frozen Lake - V0\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile. For more information visit https://gym.openai.com/envs/FrozenLake-v0/."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 1. Import libraries"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "source": [
    "## 2. Parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 20000 # Number of episodes (epochs)\n",
    "steps = 100 # Number of steps (iterations) per episodes\n",
    "lr = 0.1 # Learning rate\n",
    "gamma = 0.99  # Discount rate\n",
    "exp_rate = 1  # Epsilon (Expectation rate)\n",
    "exp_decay = 0.001 # Expecation rate decay"
   ]
  },
  {
   "source": [
    "## 3. Initialization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "action_space = env.action_space.n # Number of possible actions\n",
    "state_space = env.observation_space.n # Number of existing spaces\n",
    "q_table = np.zeros((state_space, action_space)) # Init Q-Table to all zeros"
   ]
  },
  {
   "source": [
    "## 4. Training (Iterative Q-Learning)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20000/20000 [00:15<00:00, 1306.63it/s]\n"
     ]
    }
   ],
   "source": [
    " rewards = [] # all rewards throughout different episodes\n",
    " for e in trange(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    reward = 0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        thresh = random.uniform(0, 1)  # Threshold\n",
    "        # Exploitation: Choose the most economical path\n",
    "        if thresh > exp_rate:  \n",
    "            action = np.argmax(q_table[state, :])\n",
    "        # Exploration: Discover new paths\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "        # Take a step\n",
    "        new_state, new_reward, done, info = env.step(action)\n",
    "\n",
    "        # Update Q-Table\n",
    "        # S_t * (1-a) + a * (R_t+1 + y * q_prime(S_t+1))\n",
    "        q_table[state, action] = q_table[state, action] * (1 - lr) + lr * (\n",
    "            new_reward + gamma * np.max(q_table[new_state, :])\n",
    "        )\n",
    "\n",
    "        state = new_state\n",
    "        reward += new_reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update expectation rate\n",
    "    exp_rate = 0.01 + 0.99 * np.exp(-exp_decay * e)\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "source": [
    "## 5. Analysis and Visualization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Reward for 2000: 0.121\nReward for 4000: 0.4745\nReward for 6000: 0.6385\nReward for 8000: 0.671\nReward for 10000: 0.6725\nReward for 12000: 0.663\nReward for 14000: 0.6615\nReward for 16000: 0.6835\nReward for 18000: 0.679\nReward for 20000: 0.6585\n"
     ]
    }
   ],
   "source": [
    "rewards = np.split(np.array(rewards), 10)\n",
    "for i, reward in enumerate(rewards):\n",
    "    print(f\"Reward for {(i+1)*len(reward)}: {np.mean(reward)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  (Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\nYay!\n"
     ]
    }
   ],
   "source": [
    "for e in range(6):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(f\"> EPISODE {e+1}\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    for _ in range(steps):\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.5)\n",
    "        # Only use exploitation since the Q-table is trained\n",
    "        action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Take a step\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            print(\"Yay!\" if reward else \"Game over...\")\n",
    "            time.sleep(2)\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}